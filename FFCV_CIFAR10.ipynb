{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 + MosaikML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "cuda_ver = torch.version.cuda.replace(\".\", \"\")\n",
    "\n",
    "import time\n",
    "\n",
    "import composer\n",
    "from composer.models import ComposerResNetCIFAR\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(42) # For replicability\n",
    "\n",
    "\n",
    "from composer.datasets.ffcv_utils import ffcv_monkey_patches\n",
    "from composer.datasets.ffcv_utils import write_ffcv_dataset\n",
    "\n",
    "ffcv_monkey_patches()\n",
    "\n",
    "device = \"gpu\"\n",
    "batch_size = 32\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composer.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'113'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from composer.models import ComposerResNet\n",
    "model = ComposerResNet(\n",
    "    model_name=\"resnet50\",\n",
    "    num_classes=1000,\n",
    "    pretrained=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComposerResNet(\n",
       "  (train_acc): Accuracy()\n",
       "  (val_acc): Accuracy()\n",
       "  (val_loss): CrossEntropy()\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Loader\n",
    "Next, we instantiate our CIFAR10 dataset and dataloader. We'll use the Torchvision CIFAR10 and PyTorch dataloader for the sake of familiarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:01<00:00, 114102333.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/cifar-10-python.tar.gz to /tmp\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Normalization constants\n",
    "mean = (0.507, 0.487, 0.441)\n",
    "std = (0.267, 0.256, 0.276)\n",
    "\n",
    "batch_size = 1024\n",
    "num_workers = 2\n",
    "data_directory = \"/tmp\"\n",
    "\n",
    "cifar10_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(data_directory, train=True, download=True, transform=cifar10_transforms)\n",
    "test_dataset = datasets.CIFAR10(data_directory, train=False, download=True, transform=cifar10_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               num_workers=num_workers, \n",
    "                                               batch_size=batch_size,\n",
    "                                               pin_memory=True,\n",
    "                                               drop_last=True,\n",
    "                                               shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              num_workers=num_workers, \n",
    "                                              batch_size=batch_size,\n",
    "                                              pin_memory=True,\n",
    "                                              drop_last=False,\n",
    "                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Next, we create our model. We're using composer's built-in ResNet18. To use your own custom model, please see the [custom models tutorial](https://docs.mosaicml.com/en/stable/tutorials/adding_models_datasets.html#models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposerResNetCIFAR(model_name='resnet_20', num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and Scheduler\n",
    "The trainer will handle instantiating the optimizer, but first we need to create the optimizer and LR scheduler. We're using [MosaicML's SGD with decoupled weight decay](https://arxiv.org/abs/1711.05101):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(), # Model parameters to update\n",
    "    lr=0.05, # Peak learning rate\n",
    "    momentum=0.9,\n",
    "    weight_decay=2.0e-3 # If this looks large, it's because its not scaled by the LR as in non-decoupled weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the runtime short, we'll train our baseline model for five epochs. The first epoch will be linear warmup, followed by four epochs of constant LR. We achieve this by instantiating a `LinearWithWarmupScheduler` class. Feel free to increase the number of epochs in case you want to see the impact of running it for a longer duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = composer.optim.LinearWithWarmupScheduler(\n",
    "    t_warmup=\"1ep\", # Warm up over 1 epoch\n",
    "    alpha_i=1.0, # Flat LR schedule achieved by having alpha_i == alpha_f\n",
    "    alpha_f=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a baseline model\n",
    "And now we create our trainer: Note: We want to gpu as a device because FFCV works the best on GPU-capable machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 8.4.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n",
      "\n",
      "check self.logger\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [1]:  self.logger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: <composer.loggers.logger.Logger at 0x7f126e5402e0>\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [2]:  model.logger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [3]:  model.logger = self.logger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [4]:  type(model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: composer.models.resnet_cifar.model.ComposerResNetCIFAR\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [5]:  model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: \n",
      "ComposerResNetCIFAR(\n",
      "  (train_acc): Accuracy()\n",
      "  (val_acc): Accuracy()\n",
      "  (val_loss): CrossEntropy()\n",
      "  (module): ResNetCIFAR(\n",
      "    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (blocks): Sequential(\n",
      "      (0): Block(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (criterion): CrossEntropyLoss()\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_40295/1349272072.py:4\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5ep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Train for 5 epochs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mcomposer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedulers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/composer/trainer/trainer.py:874\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, train_dataloader, train_dataloader_label, train_subset_num_batches, compute_training_metrics, max_duration, algorithms, optimizers, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_interval, eval_subset_num_batches, callbacks, loggers, run_name, progress_bar, log_to_console, console_log_level, console_stream, load_path, load_object_store, load_weights_only, load_strict_model_weights, load_progress_bar, load_ignore_keys, save_folder, save_filename, save_artifact_name, save_latest_filename, save_latest_artifact_name, save_overwrite, save_interval, save_weights_only, save_num_checkpoints_to_keep, autoresume, deepspeed_config, device, precision, grad_accum, seed, deterministic_mode, dist_timeout, ddp_sync_strategy, grad_clip_norm, profiler)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;66;03m# Set the logger\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embed\n\u001b[0;32m--> 874\u001b[0m \u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheck self.logger\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m model\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# Run Event.INIT\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/IPython/terminal/embed.py:399\u001b[0m, in \u001b[0;36membed\u001b[0;34m(header, compile_flags, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    397\u001b[0m shell \u001b[38;5;241m=\u001b[39m InteractiveShellEmbed\u001b[38;5;241m.\u001b[39minstance(_init_location_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    398\u001b[0m     frame\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, frame\u001b[38;5;241m.\u001b[39mf_lineno), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 399\u001b[0m \u001b[43mshell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstack_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_flags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_call_location_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_lineno\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m InteractiveShellEmbed\u001b[38;5;241m.\u001b[39mclear_instance()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m#restore previous instance\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/IPython/terminal/embed.py:245\u001b[0m, in \u001b[0;36mInteractiveShellEmbed.__call__\u001b[0;34m(self, header, local_ns, module, dummy, stack_depth, compile_flags, **kw)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_banner()\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Call the embedding code with a stack depth of 1 so it can skip over\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# our call and get the original caller's namespaces.\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstack_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_flags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_flags\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbanner2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_banner2\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexit_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/IPython/terminal/embed.py:337\u001b[0m, in \u001b[0;36mInteractiveShellEmbed.mainloop\u001b[0;34m(self, local_ns, module, stack_depth, compile_flags)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_completer_frame()\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_trap:\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# now, purge out the local namespace of IPython's hidden variables.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_ns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/IPython/terminal/interactiveshell.py:665\u001b[0m, in \u001b[0;36mTerminalInteractiveShell.interact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseparate_in, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 665\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_for_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfirm_exit) \\\n\u001b[1;32m    668\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_yes_no(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDo you really want to exit ([y]/n)?\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/IPython/terminal/interactiveshell.py:424\u001b[0m, in \u001b[0;36mTerminalInteractiveShell.init_prompt_toolkit_cli.<locals>.prompt\u001b[0;34m()\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt\u001b[39m():\n\u001b[1;32m    423\u001b[0m     prompt_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39min_prompt_tokens())\n\u001b[0;32m--> 424\u001b[0m     lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    425\u001b[0m     prompt_continuation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39mcontinuation_prompt_tokens())\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_complete(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincomplete\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/IPython/utils/py3compat.py:48\u001b[0m, in \u001b[0;36minput\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput\u001b[39m(prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuiltin_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/ipykernel/kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[0;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/ipykernel/kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "train_epochs = \"5ep\" # Train for 5 epochs\n",
    "device = \"gpu\"\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=lr_scheduler,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train and measure the training time below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch     0 train 100%|█████████████████████████| 48/48 [00:11<00:00,  4.16ba/s, loss/train=1.5926]         \n",
      "\n",
      "Epoch     0 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     0 val    10%|██▌                      | 1/10 [00:00<00:02,  3.13ba/s]         \u001b[A\n",
      "Epoch     0 val    30%|███████▌                 | 3/10 [00:00<00:00,  7.57ba/s]         \u001b[A\n",
      "Epoch     0 val    50%|████████████▌            | 5/10 [00:00<00:00,  9.62ba/s]         \u001b[A\n",
      "Epoch     0 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 10.85ba/s]         \u001b[A\n",
      "Epoch     0 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 11.66ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.66ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.66ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 10/10 [00:01<00:00,  9.98ba/s, metrics/eval/Accuracy=0.2631]         \u001b[A\n",
      "Epoch     1 train 100%|█████████████████████████| 48/48 [00:04<00:00, 10.62ba/s, loss/train=1.1688]         \n",
      "\n",
      "Epoch     1 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     1 val    10%|██▌                      | 1/10 [00:00<00:02,  3.25ba/s]         \u001b[A\n",
      "Epoch     1 val    30%|███████▌                 | 3/10 [00:00<00:00,  7.35ba/s]         \u001b[A\n",
      "Epoch     1 val    50%|████████████▌            | 5/10 [00:00<00:00,  9.51ba/s]         \u001b[A\n",
      "Epoch     1 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 10.79ba/s]         \u001b[A\n",
      "Epoch     1 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 11.64ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.64ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.64ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 10/10 [00:00<00:00, 10.07ba/s, metrics/eval/Accuracy=0.4884]         \u001b[A\n",
      "Epoch     2 train 100%|█████████████████████████| 48/48 [00:04<00:00, 10.38ba/s, loss/train=1.0110]         \n",
      "\n",
      "Epoch     2 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     2 val    10%|██▌                      | 1/10 [00:00<00:02,  3.16ba/s]         \u001b[A\n",
      "Epoch     2 val    30%|███████▌                 | 3/10 [00:00<00:00,  7.17ba/s]         \u001b[A\n",
      "Epoch     2 val    50%|████████████▌            | 5/10 [00:00<00:00,  9.23ba/s]         \u001b[A\n",
      "Epoch     2 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 10.74ba/s]         \u001b[A\n",
      "Epoch     2 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 11.56ba/s]         \u001b[A\n",
      "Epoch     2 val   100%|█████████████████████████| 10/10 [00:01<00:00, 11.56ba/s]         \u001b[A\n",
      "Epoch     2 val   100%|█████████████████████████| 10/10 [00:01<00:00, 11.56ba/s]         \u001b[A\n",
      "Epoch     2 val   100%|█████████████████████████| 10/10 [00:01<00:00,  9.83ba/s, metrics/eval/Accuracy=0.5027]         \u001b[A\n",
      "Epoch     3 train 100%|█████████████████████████| 48/48 [00:04<00:00, 10.47ba/s, loss/train=0.8583]         \n",
      "\n",
      "Epoch     3 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     3 val    10%|██▌                      | 1/10 [00:00<00:02,  3.15ba/s]         \u001b[A\n",
      "Epoch     3 val    30%|███████▌                 | 3/10 [00:00<00:00,  7.09ba/s]         \u001b[A\n",
      "Epoch     3 val    50%|████████████▌            | 5/10 [00:00<00:00,  9.29ba/s]         \u001b[A\n",
      "Epoch     3 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 10.50ba/s]         \u001b[A\n",
      "Epoch     3 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 11.38ba/s]         \u001b[A\n",
      "Epoch     3 val   100%|█████████████████████████| 10/10 [00:01<00:00, 11.38ba/s]         \u001b[A\n",
      "Epoch     3 val   100%|█████████████████████████| 10/10 [00:01<00:00, 11.38ba/s]         \u001b[A\n",
      "Epoch     3 val   100%|█████████████████████████| 10/10 [00:01<00:00,  9.84ba/s, metrics/eval/Accuracy=0.5542]         \u001b[A\n",
      "Epoch     4 train 100%|█████████████████████████| 48/48 [00:04<00:00, 10.47ba/s, loss/train=0.7714]         \n",
      "\n",
      "Epoch     4 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     4 val    10%|██▌                      | 1/10 [00:00<00:02,  3.30ba/s]         \u001b[A\n",
      "Epoch     4 val    30%|███████▌                 | 3/10 [00:00<00:00,  7.43ba/s]         \u001b[A\n",
      "Epoch     4 val    50%|████████████▌            | 5/10 [00:00<00:00,  9.55ba/s]         \u001b[A\n",
      "Epoch     4 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 10.89ba/s]         \u001b[A\n",
      "Epoch     4 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 11.63ba/s]         \u001b[A\n",
      "Epoch     4 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.63ba/s]         \u001b[A\n",
      "Epoch     4 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.63ba/s]         \u001b[A\n",
      "Epoch     4 val   100%|█████████████████████████| 10/10 [00:00<00:00, 10.03ba/s, metrics/eval/Accuracy=0.6180]         \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 35.3331 seconds to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use FFCV dataloaders to Speed Up Training\n",
    "Next, we convert dataset to a format used by FFCV. FFCV uses it's own data format suitable for faster dataloading. Once this cell executes successfuly, you can find ```cifar_train.ffcv``` and ```cifar_val.ffcv``` in ```data_directory``` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 99152.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 99664.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from composer.datasets.ffcv_utils import write_ffcv_dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "# Train dataset\n",
    "ds = CIFAR10(root=data_directory, train=True, download=True)\n",
    "write_ffcv_dataset(dataset=ds, write_path=data_directory + \"/cifar_train.ffcv\")\n",
    "\n",
    "# validation dataset\n",
    "ds = CIFAR10(root=data_directory, train=False, download=True)\n",
    "write_ffcv_dataset(dataset=ds, write_path=data_directory + \"/cifar_val.ffcv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current version of ffcv (0.0.3) has a bug where calling [len(dataloader) does shuffling](https://github.com/libffcv/ffcv/issues/163) of image indices to load, therefore, calls to len are expensive. Composer calls len(dataloader) function in training loop for every batch and, hence, this is a performance hit. We fix it by patching the len function using ffcv_monkey_patches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.datasets.ffcv_utils import ffcv_monkey_patches\n",
    "ffcv_monkey_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now let us construct FFCV train and test dataloaders. We use the similar transformations as used for TorchVision datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffcv\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "\n",
    "# Please note that this mean/std is different from the mean/std used for regular PyTorch dataloader as\n",
    "# ToTensor does the normalization for PyTorch dataloaders.\n",
    "cifar10_mean_ffcv = np.array([125.307, 122.961, 113.8575])\n",
    "cifar10_std_ffcv = np.array([51.5865, 50.847, 51.255])\n",
    "label_pipeline = [IntDecoder(), ffcv.transforms.ToTensor(), ffcv.transforms.Squeeze()]\n",
    "image_pipeline = [SimpleRGBImageDecoder(), ffcv.transforms.ToTensor(),\n",
    "                ffcv.transforms.ToTorchImage(channels_last=False, convert_back_int16=False),\n",
    "                ffcv.transforms.Convert(torch.float32),\n",
    "                transforms.Normalize(cifar10_mean_ffcv, cifar10_std_ffcv),\n",
    "            ]\n",
    "\n",
    "ffcv_train_dataloader = ffcv.Loader(\n",
    "                data_directory + \"/cifar_train.ffcv\",\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                order=ffcv.loader.OrderOption.RANDOM,\n",
    "                pipelines={\n",
    "                    'image': image_pipeline,\n",
    "                    'label': label_pipeline\n",
    "                },\n",
    "                drop_last=True,\n",
    "            )\n",
    "ffcv_test_dataloader = ffcv.Loader(\n",
    "                data_directory + \"/cifar_val.ffcv\",\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                order=ffcv.loader.OrderOption.RANDOM,\n",
    "                pipelines={\n",
    "                    'image': image_pipeline,\n",
    "                    'label': label_pipeline\n",
    "                },\n",
    "                drop_last=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now let's instantiate our model, optimizer, and trainer again but with FFCV dataloaders. No need to instantiate our scheduler again because it's stateless!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposerResNetCIFAR(model_name=\"resnet_20\", num_classes=10)\n",
    "\n",
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(),\n",
    "    lr=0.05,\n",
    "    momentum=0.9,\n",
    "    weight_decay=2.0e-3\n",
    ")\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=ffcv_train_dataloader,\n",
    "    eval_dataloader=ffcv_test_dataloader,\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=lr_scheduler,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "And let's get training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch     0 train 100%|█████████████████████████| 48/48 [00:05<00:00,  9.18ba/s, loss/train=1.4949]         \n",
      "\n",
      "Epoch     0 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     0 val    20%|█████                    | 2/10 [00:00<00:00, 19.85ba/s]         \u001b[A\n",
      "Epoch     0 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 37.01ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 10/10 [00:00<00:00, 37.01ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 10/10 [00:00<00:00, 37.01ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 10/10 [00:00<00:00, 37.33ba/s, metrics/eval/Accuracy=0.3116]         \u001b[A\n",
      "Epoch     1 train 100%|█████████████████████████| 48/48 [00:04<00:00, 10.94ba/s, loss/train=1.1486]         \n",
      "\n",
      "Epoch     1 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     1 val    30%|███████▌                 | 3/10 [00:00<00:00, 25.32ba/s]         \u001b[A\n",
      "Epoch     1 val    80%|████████████████████     | 8/10 [00:00<00:00, 38.22ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 10/10 [00:00<00:00, 38.22ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 10/10 [00:00<00:00, 38.22ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 10/10 [00:00<00:00, 37.46ba/s, metrics/eval/Accuracy=0.4456]         \u001b[A\n",
      "Epoch     2 train 100%|█████████████████████████| 48/48 [00:04<00:00, 11.04ba/s, loss/train=1.0350]         \n",
      "\n",
      "Epoch     2 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     2 val    30%|███████▌                 | 3/10 [00:00<00:00, 25.77ba/s]         \u001b[A\n",
      "Epoch     2 val    80%|████████████████████     | 8/10 [00:00<00:00, 38.35ba/s]         \u001b[A\n",
      "Epoch     2 val   100%|█████████████████████████| 10/10 [00:00<00:00, 38.35ba/s]         \u001b[A\n",
      "Epoch     2 val   100%|█████████████████████████| 10/10 [00:00<00:00, 38.35ba/s]         \u001b[A\n",
      "Epoch     2 val   100%|█████████████████████████| 10/10 [00:00<00:00, 37.63ba/s, metrics/eval/Accuracy=0.5473]         \u001b[A\n",
      "Epoch     3 train 100%|█████████████████████████| 48/48 [00:04<00:00, 11.02ba/s, loss/train=0.8283]         \n",
      "\n",
      "Epoch     3 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     3 val    20%|█████                    | 2/10 [00:00<00:00, 17.86ba/s]         \u001b[A\n",
      "Epoch     3 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 35.06ba/s]         \u001b[A\n",
      "Epoch     3 val   100%|█████████████████████████| 10/10 [00:00<00:00, 35.06ba/s]         \u001b[A\n",
      "Epoch     3 val   100%|█████████████████████████| 10/10 [00:00<00:00, 35.06ba/s]         \u001b[A\n",
      "Epoch     3 val   100%|█████████████████████████| 10/10 [00:00<00:00, 35.53ba/s, metrics/eval/Accuracy=0.4380]         \u001b[A\n",
      "Epoch     4 train 100%|█████████████████████████| 48/48 [00:04<00:00, 11.02ba/s, loss/train=0.7933]         \n",
      "\n",
      "Epoch     4 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     4 val    30%|███████▌                 | 3/10 [00:00<00:00, 26.25ba/s]         \u001b[A\n",
      "Epoch     4 val    80%|████████████████████     | 8/10 [00:00<00:00, 37.99ba/s]         \u001b[A\n",
      "Epoch     4 val   100%|█████████████████████████| 10/10 [00:00<00:00, 37.99ba/s]         \u001b[A\n",
      "Epoch     4 val   100%|█████████████████████████| 10/10 [00:00<00:00, 37.99ba/s]         \u001b[A\n",
      "Epoch     4 val   100%|█████████████████████████| 10/10 [00:00<00:00, 37.41ba/s, metrics/eval/Accuracy=0.5451]         \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 24.8695 seconds to train with FFCV dataloaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit()\n",
    "end_time = time.perf_counter()\n",
    "accelerated_time = end_time - start_time\n",
    "print(f\"It took {accelerated_time:0.4f} seconds to train with FFCV dataloaders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 + Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "cuda_ver = torch.version.cuda.replace(\".\", \"\")\n",
    "\n",
    "import time\n",
    "\n",
    "import composer\n",
    "from composer.models import ComposerResNetCIFAR\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(42) # For replicability\n",
    "\n",
    "\n",
    "from composer.datasets.ffcv_utils import ffcv_monkey_patches\n",
    "from composer.datasets.ffcv_utils import write_ffcv_dataset\n",
    "\n",
    "ffcv_monkey_patches()\n",
    "\n",
    "device = \"gpu\"\n",
    "batch_size = 32\n",
    "num_workers = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50864/3382758065.py:9: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n",
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/pl_bolts/utils/warnings.py:30: UserWarning: You want to use `wandb` which is not installed yet, install it with `pip install wandb`.\n",
      "  stdout_func(\n",
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/pl_bolts/utils/warnings.py:30: UserWarning: You want to use `gym` which is not installed yet, install it with `pip install gym`.\n",
      "  stdout_func(\n",
      "Global seed set to 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "# import seaborn as sn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from IPython.core.display import display\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "seed_everything(7)\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:60: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:64: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:68: LightningDeprecationWarning: DataModule property `test_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "train_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cifar10_dm = CIFAR10DataModule(\n",
    "    data_dir=PATH_DATASETS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    val_transforms=test_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, lr=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // BATCH_SIZE\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Epoch 0:   0%|          | 0/197 [00:00<?, ?it/s]                           Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Epoch 0:  76%|███████▌  | 150/197 [00:16<00:05,  9.34it/s, loss=1.48, v_num=20]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[AUnable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "\n",
      "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  81%|████████  | 160/197 [00:19<00:04,  8.38it/s, loss=1.48, v_num=20]\n",
      "Validation DataLoader 0:  25%|██▌       | 10/40 [00:00<00:00, 35.93it/s]\u001b[A\n",
      "Epoch 0:  86%|████████▋ | 170/197 [00:19<00:03,  8.78it/s, loss=1.48, v_num=20]\n",
      "Validation DataLoader 0:  50%|█████     | 20/40 [00:00<00:00, 38.04it/s]\u001b[A\n",
      "Epoch 0:  91%|█████████▏| 180/197 [00:19<00:01,  9.17it/s, loss=1.48, v_num=20]\n",
      "Validation DataLoader 0:  75%|███████▌  | 30/40 [00:00<00:00, 38.82it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▋| 190/197 [00:19<00:00,  9.56it/s, loss=1.48, v_num=20]\n",
      "Validation DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 40.14it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 197/197 [00:20<00:00,  9.83it/s, loss=1.44, v_num=20, val_loss=1.850, val_acc=0.385]\n",
      "Epoch 1:   0%|          | 0/197 [00:00<?, ?it/s, loss=1.44, v_num=20, val_loss=1.850, val_acc=0.385]          Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Epoch 1:  76%|███████▌  | 150/197 [00:16<00:05,  9.13it/s, loss=1.03, v_num=20, val_loss=1.850, val_acc=0.385]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[AUnable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "\n",
      "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  81%|████████  | 160/197 [00:19<00:04,  8.19it/s, loss=1.03, v_num=20, val_loss=1.850, val_acc=0.385]\n",
      "Validation DataLoader 0:  25%|██▌       | 10/40 [00:00<00:00, 33.94it/s]\u001b[A\n",
      "Epoch 1:  86%|████████▋ | 170/197 [00:19<00:03,  8.58it/s, loss=1.03, v_num=20, val_loss=1.850, val_acc=0.385]\n",
      "Validation DataLoader 0:  50%|█████     | 20/40 [00:00<00:00, 37.22it/s]\u001b[A\n",
      "Epoch 1:  91%|█████████▏| 180/197 [00:20<00:01,  8.97it/s, loss=1.03, v_num=20, val_loss=1.850, val_acc=0.385]\n",
      "Validation DataLoader 0:  75%|███████▌  | 30/40 [00:00<00:00, 37.87it/s]\u001b[A\n",
      "Epoch 1:  96%|█████████▋| 190/197 [00:20<00:00,  9.35it/s, loss=1.03, v_num=20, val_loss=1.850, val_acc=0.385]\n",
      "Validation DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 40.17it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 197/197 [00:20<00:00,  9.62it/s, loss=1.01, v_num=20, val_loss=1.550, val_acc=0.493]\n",
      "Epoch 2:   0%|          | 0/197 [00:00<?, ?it/s, loss=1.01, v_num=20, val_loss=1.550, val_acc=0.493]          Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Epoch 2:  76%|███████▌  | 150/197 [00:16<00:05,  9.12it/s, loss=0.724, v_num=20, val_loss=1.550, val_acc=0.493]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[AUnable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "\n",
      "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  81%|████████  | 160/197 [00:19<00:04,  8.22it/s, loss=0.724, v_num=20, val_loss=1.550, val_acc=0.493]\n",
      "Validation DataLoader 0:  25%|██▌       | 10/40 [00:00<00:00, 36.52it/s]\u001b[A\n",
      "Epoch 2:  86%|████████▋ | 170/197 [00:19<00:03,  8.62it/s, loss=0.724, v_num=20, val_loss=1.550, val_acc=0.493]\n",
      "Validation DataLoader 0:  50%|█████     | 20/40 [00:00<00:00, 38.29it/s]\u001b[A\n",
      "Epoch 2:  91%|█████████▏| 180/197 [00:19<00:01,  9.02it/s, loss=0.724, v_num=20, val_loss=1.550, val_acc=0.493]\n",
      "Validation DataLoader 0:  75%|███████▌  | 30/40 [00:00<00:00, 38.57it/s]\u001b[A\n",
      "Epoch 2:  96%|█████████▋| 190/197 [00:20<00:00,  9.40it/s, loss=0.724, v_num=20, val_loss=1.550, val_acc=0.493]\n",
      "Validation DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 40.87it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 197/197 [00:20<00:00,  9.67it/s, loss=0.706, v_num=20, val_loss=0.705, val_acc=0.755]\n",
      "Epoch 2: 100%|██████████| 197/197 [00:20<00:00,  9.57it/s, loss=0.706, v_num=20, val_loss=0.705, val_acc=0.755]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:126: LightningDeprecationWarning: DataModule property `test_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 39.77it/s]Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.7427999973297119\n",
      "        test_loss           0.7317133545875549\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.7317133545875549, 'test_acc': 0.7427999973297119}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LitResnet(lr=0.05)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    logger=CSVLogger(save_dir=\"logs/\"),\n",
    "    callbacks=[LearningRateMonitor(logging_interval=\"step\"), TQDMProgressBar(refresh_rate=10)],\n",
    ")\n",
    "\n",
    "trainer.fit(model, cifar10_dm)\n",
    "trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr-SGD</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.025408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.809310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.071421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.716269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.099457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr-SGD  train_loss  val_loss  val_acc  test_loss  test_acc\n",
       "epoch                                                              \n",
       "NaN    0.025408         NaN       NaN      NaN        NaN       NaN\n",
       "0.0         NaN    1.809310       NaN      NaN        NaN       NaN\n",
       "NaN    0.071421         NaN       NaN      NaN        NaN       NaN\n",
       "0.0         NaN    1.716269       NaN      NaN        NaN       NaN\n",
       "NaN    0.099457         NaN       NaN      NaN        NaN       NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
    "del metrics[\"step\"]\n",
    "metrics.set_index(\"epoch\", inplace=True)\n",
    "display(metrics.dropna(axis=1, how=\"all\").head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 + PL + FFCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30200/50000 [00:00<00:00, 301528.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 99600.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 99269.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n",
      "Unable to join threads to shut down before fork(). This can break multithreading in child processes.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from composer.datasets.ffcv_utils import write_ffcv_dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from composer.datasets.ffcv_utils import ffcv_monkey_patches\n",
    "from composer.datasets.ffcv_utils import write_ffcv_dataset\n",
    "ffcv_monkey_patches()\n",
    "\n",
    "data_directory = \"/tmp\"\n",
    "\n",
    "# Train dataset\n",
    "ds = CIFAR10(root=data_directory, train=True, download=True)\n",
    "write_ffcv_dataset(dataset=ds, write_path=data_directory + \"/cifar_train.ffcv\")\n",
    "\n",
    "# validation dataset\n",
    "ds = CIFAR10(root=data_directory, train=False, download=True)\n",
    "write_ffcv_dataset(dataset=ds, write_path=data_directory + \"/cifar_val.ffcv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffcv\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "\n",
    "# Please note that this mean/std is different from the mean/std used for regular PyTorch dataloader as\n",
    "# ToTensor does the normalization for PyTorch dataloaders.\n",
    "cifar10_mean_ffcv = np.array([125.307, 122.961, 113.8575])\n",
    "cifar10_std_ffcv = np.array([51.5865, 50.847, 51.255])\n",
    "label_pipeline = [IntDecoder(), ffcv.transforms.ToTensor(), ffcv.transforms.Squeeze()]\n",
    "image_pipeline = [SimpleRGBImageDecoder(), ffcv.transforms.ToTensor(),\n",
    "                ffcv.transforms.ToTorchImage(channels_last=False, convert_back_int16=False),\n",
    "                ffcv.transforms.Convert(torch.float32),\n",
    "                transforms.Normalize(cifar10_mean_ffcv, cifar10_std_ffcv),\n",
    "            ]\n",
    "\n",
    "ffcv_train_dataloader = ffcv.Loader(\n",
    "                data_directory + \"/cifar_train.ffcv\",\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                order=ffcv.loader.OrderOption.RANDOM,\n",
    "                pipelines={\n",
    "                    'image': image_pipeline,\n",
    "                    'label': label_pipeline\n",
    "                },\n",
    "                drop_last=True,\n",
    "            )\n",
    "ffcv_val_dataloader = ffcv.Loader(\n",
    "                data_directory + \"/cifar_val.ffcv\",\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                order=ffcv.loader.OrderOption.SEQUENTIAL,\n",
    "                pipelines={\n",
    "                    'image': image_pipeline,\n",
    "                    'label': label_pipeline\n",
    "                },\n",
    "                drop_last=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  28%|██▊       | 520/1875 [00:10<00:26, 51.80it/s, loss=1.51, v_num=21]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to step 527 times. The specified number of total steps is 525",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LitResnet(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      5\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     num_sanity_val_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffcv_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffcv_val_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    807\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    809\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    810\u001b[0m )\n\u001b[0;32m--> 811\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1236\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1238\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1353\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:266\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(\n\u001b[1;32m    263\u001b[0m     dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_strategy_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataloader_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    264\u001b[0m )\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:214\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_lr_schedulers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_plateau_schedulers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_ready_batches_reached():\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_lr_schedulers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, update_plateau_schedulers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:432\u001b[0m, in \u001b[0;36mTrainingEpochLoop.update_lr_schedulers\u001b[0;34m(self, interval, update_plateau_schedulers)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    429\u001b[0m active_optimizers \u001b[38;5;241m=\u001b[39m _get_active_optimizers(\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizer_frequencies, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_batch_idx\n\u001b[1;32m    431\u001b[0m )\n\u001b[0;32m--> 432\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_learning_rates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_plateau_schedulers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_plateau_schedulers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactive_optimizers\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:493\u001b[0m, in \u001b[0;36mTrainingEpochLoop._update_learning_rates\u001b[0;34m(self, interval, update_plateau_schedulers, opt_indices)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_progress\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# update LR\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr_scheduler_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_progress\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1595\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1592\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1595\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py:1560\u001b[0m, in \u001b[0;36mLightningModule.lr_scheduler_step\u001b[0;34m(self, scheduler, optimizer_idx, metric)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;124;03mOverride this method to adjust the default way the\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;124;03m:class:`~pytorch_lightning.trainer.trainer.Trainer` calls each scheduler.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1560\u001b[0m     \u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1562\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(metric)\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:154\u001b[0m, in \u001b[0;36m_LRScheduler.step\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 154\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(EPOCH_DEPRECATION_WARNING, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:1593\u001b[0m, in \u001b[0;36mOneCycleLR.get_lr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1590\u001b[0m step_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_epoch\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps:\n\u001b[0;32m-> 1593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m times. The specified number of total steps is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1594\u001b[0m                      \u001b[38;5;241m.\u001b[39mformat(step_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps))\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m   1597\u001b[0m     start_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Tried to step 527 times. The specified number of total steps is 525"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  28%|██▊       | 520/1875 [00:23<01:00, 22.31it/s, loss=1.51, v_num=21]"
     ]
    }
   ],
   "source": [
    "model = LitResnet(lr=0.5)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    logger=CSVLogger(save_dir=\"logs/\"),\n",
    "    callbacks=[ TQDMProgressBar(refresh_rate=10)],\n",
    "    num_sanity_val_steps=0\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=ffcv_train_dataloader, val_dataloaders=ffcv_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv2",
   "language": "python",
   "name": "ffcv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
