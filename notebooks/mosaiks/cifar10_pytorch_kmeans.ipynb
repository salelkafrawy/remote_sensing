{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer custom CNN with MOSAIKS+KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import inspect\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from time import time\n",
    "from kmeans_pytorch import kmeans, kmeans_predict\n",
    "\n",
    "CURR_DIR = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "PARENT_DIR = os.path.dirname(CURR_DIR)\n",
    "sys.path.insert(0, \"/home/mila/s/sara.ebrahim-elkafrawy/scratch/ecosystem_project/remote_sensing\")\n",
    "\n",
    "from dataset.pytorch_dataset import GeoLifeCLEF2022Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from mosaiks_utils import visualize_3d_patches, visTensor, normalize_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(0)\n",
    "# dimensions, num clusters\n",
    "dims, num_clusters = 3, 64\n",
    "\n",
    "# data sizes\n",
    "data_sizes = [1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data size: 1000000\n",
      "running k-means on cpu..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 189it [02:38,  1.19it/s, center_shift=0.000092, iteration=189, tol=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu time: 158.4894609451294\n",
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 187it [02:30,  1.24it/s, center_shift=0.000094, iteration=187, tol=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu time: 150.9328956604004\n"
     ]
    }
   ],
   "source": [
    "gpu_times = []\n",
    "cpu_times = []\n",
    "\n",
    "for data_size in data_sizes:\n",
    "    print(f'\\ndata size: {data_size}')\n",
    "\n",
    "    # data\n",
    "    x = np.random.randn(data_size, dims) / 6\n",
    "    x = torch.from_numpy(x)\n",
    "\n",
    "        # cpu\n",
    "    start_cpu = time()\n",
    "    kmeans_cpu = kmeans(X=x, num_clusters=num_clusters, device=torch.device('cpu'))\n",
    "    cpu_time = time() - start_cpu\n",
    "    cpu_times.append(cpu_time)\n",
    "    print(f'cpu time: {cpu_time}')\n",
    "    \n",
    "    # gpu\n",
    "    start_gpu = time()\n",
    "    kmeans_gpu = kmeans(X=x, num_clusters=num_clusters, device=torch.device('cuda:0'))\n",
    "    gpu_time = time() - start_gpu\n",
    "    gpu_times.append(gpu_time)\n",
    "    print(f'gpu time: {gpu_time}')\n",
    "    \n",
    "\n",
    "    start_cpu = time()\n",
    "    kmeans_sklearn = KMeans(n_clusters=num_clusters, \n",
    "                    random_state=random_state,\n",
    "                    max_iter=50,\n",
    "                    verbose=False)\n",
    "    kmeans_geo = kmeans_sklearn.fit(x)\n",
    "    cpu_time = time() - start_cpu\n",
    "    cpu_times.append(cpu_time)\n",
    "    print(f'sklearn cpu time: {cpu_time}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trf = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=trf)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model: two-layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for the model\n",
    "dataset = 'cifar10'\n",
    "kernel_size = 8\n",
    "num_filters = 100\n",
    "mode = 'whiten' # options ['whiten', 'no_whiten']\n",
    "zca_bias = 0.001\n",
    "whiten = True\n",
    "save_path = f\"/home/mila/s/sara.ebrahim-elkafrawy/scratch/ecosystem_project/ckpts/{dataset}_two_layer_mosaiks_kmeans_{kernel_size}_{mode}_zca_{zca_bias}.pt\"\n",
    "max_iter = 100\n",
    "random_state = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/nn/modules/conv.py:453: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352660876/work/aten/src/ATen/native/Convolution.cpp:882.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=3, out_channels=num_filters, kernel_size=kernel_size, padding='same', bias=True),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "      nn.Conv2d(in_channels=num_filters, out_channels=64, kernel_size=7, padding='same', bias=True),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "    \n",
    "      nn.AdaptiveAvgPool2d(9),\n",
    "    \n",
    "      nn.Flatten(),\n",
    "      nn.Dropout(0.5),\n",
    "      nn.Linear(5184, 512), #50176\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512, len(classes))\n",
    "      ) \n",
    "model(torch.rand((1, 3, 224, 224))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Mosaiks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#       nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, padding='same', bias=True),\n",
    "#       nn.LeakyReLU(),\n",
    "#       nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "#       nn.Conv2d(in_channels=64, out_channels=64, kernel_size=7, padding='same', bias=True),\n",
    "#       nn.LeakyReLU(),\n",
    "#       nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "#       nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same', bias=True),\n",
    "#       nn.LeakyReLU(),\n",
    "#       nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "#       nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding='same', bias=True),\n",
    "#       nn.LeakyReLU(),\n",
    "#       nn.MaxPool2d(2, stride=2),\n",
    "    \n",
    "#       nn.AdaptiveAvgPool2d(9),\n",
    "    \n",
    "#       nn.Flatten(),\n",
    "#       nn.Dropout(0.5),\n",
    "#       nn.Linear(20736, 512), #50176\n",
    "#       nn.ReLU(),\n",
    "#       nn.Linear(512, num_species)\n",
    "#       ) \n",
    "# model(torch.rand((1, 3, 224, 224))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## slicing up the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 100, kernel_size=(8, 8), stride=(1, 1), padding=same)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(100, 64, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "  (4): LeakyReLU(negative_slope=0.01)\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): AdaptiveAvgPool2d(output_size=9)\n",
       "  (7): Flatten(start_dim=1, end_dim=-1)\n",
       "  (8): Dropout(p=0.5, inplace=False)\n",
       "  (9): Linear(in_features=5184, out_features=512, bias=True)\n",
       "  (10): ReLU()\n",
       "  (11): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually set the indices of all convolution layers and the afterwards activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2-conv layers\n",
    "conv_lyrs = [0, 3]\n",
    "act_lyrs = [1, 4]\n",
    "\n",
    "# for 4-conv layers\n",
    "# conv_lyrs = [0, 2, 4, 6]\n",
    "# act_lyrs = [1, 4, 7, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight ---------------\t torch.Size([100, 3, 8, 8])\n",
      "0.bias ---------------\t torch.Size([100])\n",
      "3.weight ---------------\t torch.Size([64, 100, 7, 7])\n",
      "3.bias ---------------\t torch.Size([64])\n",
      "9.weight ---------------\t torch.Size([512, 5184])\n",
      "9.bias ---------------\t torch.Size([512])\n",
      "11.weight ---------------\t torch.Size([10, 512])\n",
      "11.bias ---------------\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, '---------------\\t', param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks for activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dim = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features_dim[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f7f0ddf3df0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 2-conv layers\n",
    "model[1].register_forward_hook(get_features('relu_layer_1'))\n",
    "model[4].register_forward_hook(get_features('relu_layer_4'))\n",
    "\n",
    "\n",
    "# for 4-conv layers (without nn.AdaptivePool layer)\n",
    "# model[1].register_forward_hook(get_features('relu_layer_1'))\n",
    "# model[4].register_forward_hook(get_features('relu_layer_4'))\n",
    "# model[7].register_forward_hook(get_features('relu_layer_7'))\n",
    "# model[10].register_forward_hook(get_features('relu_layer_10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featuremap_patches(train_dataloader, layer_idx, patch_size, max_patches, random_state):\n",
    "    '''expects image shape of (width, height, n_channels)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_dataloader: Dataset\n",
    "                   dataloader for the training dataset to extract from\n",
    "    \n",
    "    layer_idx: int\n",
    "               The index of the current feature map. \n",
    "               conv_lyrs[layer_idx] should return the real index of the current weights/parameter \n",
    "               \n",
    "    patch_size: (int, int)\n",
    "                Receptive field size or width and height of the extracted patch.\n",
    "                \n",
    "    max_patches: int\n",
    "                 The number of extracted patches.\n",
    "    \n",
    "    random_state: int\n",
    "                  Random state for reproducibility \n",
    "    Return\n",
    "    ------\n",
    "    patches: ndarrah\n",
    "             final patches of size (max_patches, patch_size, n_channels)\n",
    "    '''\n",
    "    all_patches = []\n",
    "    \n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        images, target = batch\n",
    "#         patches['rgb'] = trf(patches['rgb'])\n",
    "        \n",
    "        if layer_idx == 0:\n",
    "            curr_feat_map = images.numpy()\n",
    "        else:\n",
    "            output = model(images)\n",
    "            curr_feat_map = features_dim[f'relu_layer_{act_lyrs[layer_idx-1]}'].numpy()\n",
    "\n",
    "        # grab one image from the batch\n",
    "        img_idx = random.randint(0, batch_size-1)\n",
    "        if img_idx >= curr_feat_map.shape[0]:\n",
    "            img_idx = 0\n",
    "        curr_feat_map = curr_feat_map[img_idx].transpose((1,2,0))\n",
    "\n",
    "        # curr_feat_map: expected shape: (width, height, n_channels)\n",
    "        # output shape: (max_patches, patch_size, n_channels)\n",
    "        random_patches =  extract_patches_2d(curr_feat_map, \n",
    "                                 patch_size, \n",
    "                                 max_patches=max_patches,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "        random_patches = np.reshape(random_patches, (len(random_patches), -1))\n",
    "\n",
    "        all_patches.append(random_patches)\n",
    "    \n",
    "    return np.concatenate(all_patches, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the K-Means to two conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for conv layer#0\n",
      "kmeans for output of relu act layer#1\n",
      "num of patches: 1650000\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -24.313, 24.671, 0.000, 0.554\n",
      "applying kmeans ...\n",
      "(100, 192)\n",
      "Updating parameter#0 with size: torch.Size([100, 3, 8, 8])\n",
      "for conv layer#3\n",
      "kmeans for output of relu act layer#4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 11027) is killed by signal: Killed. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m max_patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_clusters\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# number of random patches to extract from each image/feature map\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Step#1: extract random patches  shape:(32064, 147)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m featmap_patches \u001b[38;5;241m=\u001b[39m \u001b[43mget_featuremap_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmax_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_patches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum of patches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(featmap_patches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Step#2: normalize and whiten the patches\u001b[39;00m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mget_featuremap_patches\u001b[0;34m(train_dataloader, layer_idx, patch_size, max_patches, random_state)\u001b[0m\n\u001b[1;32m     34\u001b[0m     curr_feat_map \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     curr_feat_map \u001b[38;5;241m=\u001b[39m features_dim[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu_layer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mact_lyrs[layer_idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# grab one image from the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ffcv2/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m callable(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 11027) is killed by signal: Killed. "
     ]
    }
   ],
   "source": [
    "for layer_idx, relu_idx in enumerate(act_lyrs):\n",
    "    if layer_idx == len(conv_lyrs):\n",
    "        break\n",
    "        \n",
    "    print(f'for conv layer#{conv_lyrs[layer_idx]}')\n",
    "    print(f'kmeans for output of relu act layer#{relu_idx}')   \n",
    "    \n",
    "    curr_param_idx = conv_lyrs[layer_idx]\n",
    "    curr_param_sz = model[curr_param_idx].weight.data.shape\n",
    "    num_clusters = curr_param_sz[0]  # number of output filters/kernels in a layer\n",
    "    num_ch = curr_param_sz[1]    # number of channels\n",
    "    patch_size = (curr_param_sz[2], curr_param_sz[3])  # filter/kernel's width and height\n",
    "    max_patches = int(num_clusters/4)  # number of random patches to extract from each image/feature map\n",
    "    \n",
    "    # Step#1: extract random patches  shape:(32064, 147)\n",
    "    featmap_patches = get_featuremap_patches(trainloader,\n",
    "                                             layer_idx=layer_idx,\n",
    "                                             patch_size=patch_size,\n",
    "                                             max_patches=max_patches,\n",
    "                                             random_state=random_state)\n",
    "    print(f'num of patches: {len(featmap_patches)}')\n",
    "    # Step#2: normalize and whiten the patches\n",
    "    whiten_patches, means, zca_mat = normalize_patches(featmap_patches, zca_bias=zca_bias, whiten=whiten)\n",
    "    print(f'whiten_patches [min],[max],[mean],[std]: {whiten_patches.min():.3f}, {whiten_patches.max():.3f}, {whiten_patches.mean():.3f}, {whiten_patches.std():.3f}')\n",
    "    \n",
    "    # Step#3: apply kmeans to the whitened patches\n",
    "    print(f'applying kmeans ...')\n",
    "    kmeans = KMeans(n_clusters=num_clusters, \n",
    "                    random_state=random_state,\n",
    "                    max_iter=max_iter,\n",
    "                    verbose=False)\n",
    "    kmeans_geo = kmeans.fit(whiten_patches)\n",
    "  \n",
    "    print(kmeans_geo.cluster_centers_.shape) # (64, 147)\n",
    "    \n",
    "#     kmeans_geo_reconstructed = np.dot(np.dot(np.linalg.inv(zca_mat), kmeans_geo.cluster_centers_.T).T, np.linalg.inv(zca_mat))\n",
    "    # change the weights of the corresponding conv layer\n",
    "    print(f'Updating parameter#{curr_param_idx} with size: {curr_param_sz}')\n",
    "    model[curr_param_idx].weight.data = torch.from_numpy(kmeans_geo.cluster_centers_.reshape(\n",
    "                                            num_clusters, \n",
    "                                            patch_size[0], \n",
    "                                            patch_size[1],\n",
    "                                            num_ch,).transpose(0, 3, 1, 2)\n",
    "                                        )\n",
    "#     norm_param = (x - x.mean())/(x.std())\n",
    "#     model[curr_param_idx].weight.data = norm_param\n",
    "    \n",
    "# save the model\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights_data\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_complex numbers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39miscomplex(weights_data\u001b[38;5;241m.\u001b[39msum())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m check_weights(\u001b[43mmodel\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def check_weights(weights_data):\n",
    "    print(f'min: {weights_data.min()}')\n",
    "    print(f'max: {weights_data.max()}')\n",
    "    print(f'mean: {weights_data.mean()}')\n",
    "    print(f'std: {weights_data.std()}')\n",
    "    print(f'num_complex numbers: {np.iscomplex(weights_data.sum())}')\n",
    "check_weights(model[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visTensor(model[0].weight.data, ch=0, nrow=10, allkernels=False)\n",
    "plt.axis('off')\n",
    "plt.ioff()\n",
    "plt.rcParams['savefig.facecolor']='black'\n",
    "plt.savefig(f'conv1_{dataset}_{str(kernel_size)}_{str(num_filters)}_{mode}_color.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_np = model[0].weight.data.numpy()\n",
    "weight_np = np.interp(weight_np, (weight_np.min(), weight_np.max()), (-3, +3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_weights(weight_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m, s = weight_np.mean(), weight_np.std()\n",
    "# weight_np = (weight_np-m)/(s+1e-9)\n",
    "check_weights(weight_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visTensor(torch.from_numpy(weight_np), ch=0, nrow=10, allkernels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv2",
   "language": "python",
   "name": "ffcv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
