{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer custom CNN from MOSAIKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GeoLife data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "CURR_DIR = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "PARENT_DIR = os.path.dirname(CURR_DIR)\n",
    "sys.path.insert(0, \"/home/mila/s/sara.ebrahim-elkafrawy/scratch/ecosystem_project/remote_sensing\")\n",
    "\n",
    "from dataset.pytorch_dataset import GeoLifeCLEF2022Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from mosaiks_utils import visualize_3d_patches, visTensor, normalize_patches, DBN\n",
    "\n",
    "random_state = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/network/scratch/s/sara.ebrahim-elkafrawy/\" #\"/network/scratch/s/sara.ebrahim-elkafrawy/small_geo_data\"  # \"/network/scratch/s/sara.ebrahim-elkafrawy/\" \n",
    "split = \"train\"\n",
    "use_ffcv_loader = False\n",
    "num_species= 17037\n",
    "bands = [\"rgb\"] \n",
    "batch_size = 1\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_train_dataset = GeoLifeCLEF2022Dataset(\n",
    "                root=data_dir,\n",
    "                subset=split,\n",
    "                use_ffcv_loader=use_ffcv_loader,\n",
    "                region=\"both\",\n",
    "                patch_data=bands,\n",
    "                use_rasters=False,\n",
    "                patch_extractor=None,\n",
    "                transform=None,\n",
    "                target_transform=None,\n",
    "                opts=None,\n",
    "            )\n",
    "\n",
    "geo_train_loader = DataLoader(\n",
    "                geo_train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "trf = torch.nn.Sequential(\n",
    "    transforms.Resize(size=(224, 224), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.Normalize((106.9413, 114.8733, 104.5285), (51.0005, 44.8595, 43.2014)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model: two-layer CNN MOSAIK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for the model\n",
    "kernel_size = 7\n",
    "mode = 'whiten_minibatch_allGeo' # options ['whiten', 'no_whiten']\n",
    "conv1_num_filters = 100\n",
    "conv2_num_filters = 64\n",
    "whiten = True\n",
    "zca_bias = 0.001\n",
    "save_path = f\"/home/mila/s/sara.ebrahim-elkafrawy/scratch/ecosystem_project/ckpts/two_layer_mosaiks_kmeans_{kernel_size}_{mode}_zcaBias_{zca_bias}_minibatch.pt\"\n",
    "max_iter = 6\n",
    "random_state = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17037])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=3, out_channels=conv1_num_filters, kernel_size=kernel_size, padding='same', bias=True),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "      nn.Conv2d(in_channels=conv1_num_filters, out_channels=conv2_num_filters, kernel_size=kernel_size, padding='same', bias=True),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "    \n",
    "      nn.AdaptiveAvgPool2d(9),\n",
    "    \n",
    "      nn.Flatten(),\n",
    "      nn.Dropout(0.5),\n",
    "      nn.Linear(5184, 512), #50176\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512, num_species)\n",
    "      ) \n",
    "model(torch.rand((1, 3, 224, 224))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Mosaiks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#       nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, padding='same', bias=True),\n",
    "#       nn.LeakyReLU(),\n",
    "#       nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "#       nn.Conv2d(in_channels=64, out_channels=64, kernel_size=7, padding='same', bias=True),\n",
    "#       nn.LeakyReLU(),\n",
    "#       nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "#       nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same', bias=True),\n",
    "#       nn.LeakyReLU(),\n",
    "#       nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "#       nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding='same', bias=True),\n",
    "#       nn.LeakyReLU(),\n",
    "#       nn.MaxPool2d(2, stride=2),\n",
    "    \n",
    "#       nn.AdaptiveAvgPool2d(9),\n",
    "    \n",
    "#       nn.Flatten(),\n",
    "#       nn.Dropout(0.5),\n",
    "#       nn.Linear(20736, 512), #50176\n",
    "#       nn.ReLU(),\n",
    "#       nn.Linear(512, num_species)\n",
    "#       ) \n",
    "# model(torch.rand((1, 3, 224, 224))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 100, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(100, 64, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "  (4): LeakyReLU(negative_slope=0.01)\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): AdaptiveAvgPool2d(output_size=9)\n",
       "  (7): Flatten(start_dim=1, end_dim=-1)\n",
       "  (8): Dropout(p=0.5, inplace=False)\n",
       "  (9): Linear(in_features=5184, out_features=512, bias=True)\n",
       "  (10): ReLU()\n",
       "  (11): Linear(in_features=512, out_features=17037, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually set the indices of all convolution layers and the afterwards activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2-conv layers\n",
    "conv_lyrs = [0, 3]\n",
    "act_lyrs = [1, 4]\n",
    "\n",
    "# for 4-conv layers\n",
    "# conv_lyrs = [0, 2, 4, 6]\n",
    "# act_lyrs = [1, 4, 7, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight ---------------\t torch.Size([100, 3, 7, 7])\n",
      "0.bias ---------------\t torch.Size([100])\n",
      "3.weight ---------------\t torch.Size([64, 100, 7, 7])\n",
      "3.bias ---------------\t torch.Size([64])\n",
      "9.weight ---------------\t torch.Size([512, 5184])\n",
      "9.bias ---------------\t torch.Size([512])\n",
      "11.weight ---------------\t torch.Size([17037, 512])\n",
      "11.bias ---------------\t torch.Size([17037])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, '---------------\\t', param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 100, kernel_size=(7, 7), stride=(1, 1), padding=same),\n",
       " LeakyReLU(negative_slope=0.01),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(100, 64, kernel_size=(7, 7), stride=(1, 1), padding=same),\n",
       " LeakyReLU(negative_slope=0.01),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " AdaptiveAvgPool2d(output_size=9),\n",
       " Flatten(start_dim=1, end_dim=-1),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " Linear(in_features=5184, out_features=512, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=512, out_features=17037, bias=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook the activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dim = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features_dim[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f6a14320a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 2-conv layers\n",
    "model[1].register_forward_hook(get_features('relu_layer_1'))\n",
    "model[4].register_forward_hook(get_features('relu_layer_4'))\n",
    "\n",
    "# for 4-conv layers (without nn.AdaptivePool layer)\n",
    "# model[1].register_forward_hook(get_features('relu_layer_1'))\n",
    "# model[4].register_forward_hook(get_features('relu_layer_4'))\n",
    "# model[7].register_forward_hook(get_features('relu_layer_7'))\n",
    "# model[10].register_forward_hook(get_features('relu_layer_10'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the K-Means MiniBatch to two conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for conv layer#0\n",
      "kmeans for output of relu act layer#1\n",
      "current parameter size: torch.Size([100, 3, 7, 7])\n",
      "num_feats:100, num_ch:3, patch_size:(7, 7)\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -12.696, 10.032, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -10.776, 11.342, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -10.479, 13.238, -0.000, 0.584\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -16.121, 11.034, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -10.589, 10.844, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -10.137, 10.725, -0.000, 0.584\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -12.227, 10.432, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -11.375, 12.156, -0.000, 0.584\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -13.732, 9.869, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -10.385, 10.649, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -11.946, 11.949, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -12.367, 11.556, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -10.852, 19.495, -0.000, 0.584\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -11.539, 9.919, -0.000, 0.584\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -14.357, 11.633, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -10.314, 13.452, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -10.483, 10.789, -0.000, 0.584\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -11.817, 10.503, -0.000, 0.584\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -13.953, 10.275, -0.000, 0.584\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n",
      "whiten_patches [min],[max],[mean],[std]: -12.524, 11.405, -0.000, 0.585\n",
      "Running Kmeans for 1322825 patch\n",
      "zca bias 0.001\n",
      "negatives eigen values: (0,)\n"
     ]
    }
   ],
   "source": [
    "all_patches = []\n",
    "params = []\n",
    "# dbn = DBN(3, 3, affine=False, momentum=1.)\n",
    "\n",
    "for layer_idx, relu_idx in enumerate(act_lyrs):\n",
    "    if layer_idx == len(conv_lyrs):\n",
    "        break\n",
    "        \n",
    "    print(f'for conv layer#{conv_lyrs[layer_idx]}')\n",
    "    print(f'kmeans for output of relu act layer#{relu_idx}')\n",
    "    \n",
    "    curr_param_idx = conv_lyrs[layer_idx]\n",
    "    curr_param_sz = model[curr_param_idx].weight.data.shape # or list(model.children())[curr_param_idx].weight\n",
    "\n",
    "    print(f'current parameter size: {curr_param_sz}')\n",
    "        \n",
    "    num_feats = curr_param_sz[0]\n",
    "    num_ch = curr_param_sz[1]\n",
    "    patch_size = (curr_param_sz[2], curr_param_sz[3])\n",
    "    num_iters = max_iter   # The online learning part: cycle over the whole dataset 6 times\n",
    "    max_patches = int(num_feats/4)\n",
    "\n",
    "    print(f'num_feats:{num_feats}, num_ch:{num_ch}, patch_size:{patch_size}')\n",
    "    \n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_feats, \n",
    "                             random_state=random_state,\n",
    "                             verbose=False)\n",
    "\n",
    "    geo_train_loader = DataLoader(\n",
    "                    geo_train_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=num_workers,\n",
    "                    shuffle=True,\n",
    "                )\n",
    "\n",
    "    index = 0\n",
    "    buffer = []\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        for batch in geo_train_loader:\n",
    "            patches, target, meta = batch\n",
    "\n",
    "            # this is trf_stand: means standardization with the GeoLife stats\n",
    "            patches['rgb'] = trf(patches['rgb'])\n",
    "            output = model(patches['rgb'])\n",
    "        \n",
    "            if layer_idx == 0:\n",
    "                curr_feats = patches['rgb'].numpy()\n",
    "                \n",
    "            else:\n",
    "                curr_feats = features_dim[f'relu_layer_{act_lyrs[layer_idx-1]}'].numpy()\n",
    "            \n",
    "            curr_feats = curr_feats.squeeze(0)\n",
    "            curr_feats = curr_feats.transpose((1,2,0))\n",
    "            \n",
    "            # expects image shape of (width, height, n_channels)\n",
    "            data = extract_patches_2d(curr_feats, \n",
    "                                      patch_size, \n",
    "                                      max_patches=max_patches,\n",
    "                                      random_state=random_state)\n",
    "#             print(data.shape) # (32, 7, 7, 3) , (32, 7, 7, 64)\n",
    "\n",
    "            data = np.reshape(data, (len(data), -1))\n",
    "            all_patches.append(data)\n",
    "            buffer.append(data)\n",
    "            \n",
    "            index += 1\n",
    "            if index % int(len(geo_train_loader)/30) == 0:   # by the end of the train loader ~2m patches\n",
    "                data = np.concatenate(buffer, axis=0)\n",
    "                if np.any(np.isnan(data)):\n",
    "                    data = np.nan_to_num(data)\n",
    "                    \n",
    "                data, means, zca_mat = normalize_patches(data, zca_bias=zca_bias, whiten=whiten)\n",
    "                print(f'whiten_patches [min],[max],[mean],[std]: {data.min():.3f}, {data.max():.3f}, {data.mean():.3f}, {data.std():.3f}')\n",
    "                \n",
    "                print(f'Running Kmeans for {len(data)} patch')\n",
    "                kmeans.partial_fit(data)\n",
    "                \n",
    "                buffer = []\n",
    "#             if index % 10000 == 0:\n",
    "#                 print(f\"Partial fit of {index} out of {num_iters * len(geo_train_loader)}\")\n",
    "\n",
    "    # change the weights of the corresponding conv layer\n",
    "    print(f'Updating parameter#{curr_param_idx} with size: {curr_param_sz}')\n",
    "    params.append(kmeans.cluster_centers_)\n",
    "    model[curr_param_idx].weight.data = torch.from_numpy(kmeans.cluster_centers_.reshape(\n",
    "                                            num_feats, \n",
    "                                            patch_size[0], \n",
    "                                            patch_size[1],\n",
    "                                            num_ch,).transpose(0, 3, 1, 2)\n",
    "                                        )\n",
    "#     norm_param = (x - x.mean())/(x.std())\n",
    "#     model[curr_param_idx].weight.data = norm_param\n",
    "\n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the filters of the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weights(weights_data):\n",
    "    print(f'min: {weights_data.min()}')\n",
    "    print(f'max: {weights_data.max()}')\n",
    "    print(f'mean: {weights_data.mean()}')\n",
    "    print(f'std: {weights_data.std()}')\n",
    "    print(f'num_complex numbers: {np.iscomplex(weights_data.sum())}')\n",
    "check_weights(model[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visTensor(model[0].weight.data, ch=0, allkernels=False)\n",
    "plt.axis('off')\n",
    "plt.ioff()\n",
    "plt.rcParams['savefig.facecolor']='black'\n",
    "plt.savefig(f'conv1_{str(kernel_size)}_{mode}_zcaBias_{zca_bias}_minibatch.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/home/mila/s/sara.ebrahim-elkafrawy/scratch/ecosystem_project/ckpts/two_layer_mosaiks_kmeans_7_whiten_minibatch.pt\"\n",
    "# model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visTensor(model[0].weight.data, ch=0, allkernels=False)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv2",
   "language": "python",
   "name": "ffcv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
