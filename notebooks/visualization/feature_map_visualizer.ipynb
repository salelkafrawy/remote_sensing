{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Feature Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources: [here](https://www.kaggle.com/code/magokecol/pytorch-feature-maps-visualizer-snake-version) and [here](https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /home/mila/s/sara.ebrahim-elkafrawy/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth\n",
      "100%|██████████| 548M/548M [00:05<00:00, 100MB/s]  \n"
     ]
    }
   ],
   "source": [
    "model = models.vgg19_bn(pretrained=True)\n",
    "model.eval()\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 3\n",
      "Layers[0]: 53\n"
     ]
    }
   ],
   "source": [
    "# list cnn layers\n",
    "layers = [layer for layer in model.children()]\n",
    "print('Layers: {}'.format(len(layers)))\n",
    "print('Layers[0]: {}'.format(len(layers[0])))\n",
    "# for l in layers:\n",
    "#     print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks Mila!\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# global variable to work with GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available(): print('Thanks Mila!')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which pixels activated that feature map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is the following: we start with a picture containing random pixels. We apply the network in evaluation mode to this random image, calculate the average activation of a certain feature map in a certain layer from which we then compute the gradients with respect to the input image pixel values. Knowing the gradients for the pixel values we then proceed to update the pixel values in a way that maximizes the average activation of the chosen feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know that this might sound confusing so let’s explain it again in different words: The network weights are fixed, the network will not be trained, and we try to find an image that maximizes the average activation of a certain feature map by performing gradient descent optimization on the pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is this good for? Let’s say we are interested in the feature maps of layer i. We register a forward hook on layer i that, once the forward method of layer i is called, saves the features of layer i in a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This class serves to hook cnn layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    def __init__(self, module, device=None):\n",
    "        # we are going to hook some model's layer (module here)\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.device = device\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        # when the module.forward() is executed, here we intercept its\n",
    "        # input and output. We are interested in the module's output.\n",
    "        self.features = output.clone()\n",
    "        if self.device is not None:\n",
    "            self.features = self.features.to(device)\n",
    "        self.features.requires_grad_(True)\n",
    "\n",
    "    def close(self):\n",
    "        # we must call this method to free memory resources\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you use a SaveFeatures object?\n",
    "\n",
    "Register your hook for layer `i` with activations = `SaveFeatures(list(self.model.children())[i])` and after you applied your model to the image with `model(img_var)` you can access the features the hook saved for us in activations.features. Remember to call the method close to free up used memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMapVisualizer():\n",
    "    def __init__(self, cnn, device, channels=3, layers_base=None, norm=None, denorm=None, save=None):\n",
    "        self.model = cnn\n",
    "\n",
    "        if layers_base is None:\n",
    "            self.layers = self.model\n",
    "        else:\n",
    "            self.layers = layers_base\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.device = device\n",
    "\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
    "            \n",
    "        self.norm = norm\n",
    "        self.denorm = denorm\n",
    "\n",
    "        if norm is None:\n",
    "            self.norm = transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "\n",
    "        if denorm is None:\n",
    "            self.denorm = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "            \n",
    "        self.save = save\n",
    "\n",
    "    def set_layers_base(self, layers):\n",
    "        # sometime we want to access to layers in deeper levels\n",
    "        # so we could call something like:\n",
    "        # featureMap.set_layers_base([module for module in model.children()][5][1])\n",
    "        self.layers = layers\n",
    "        \n",
    "    def optimize_img(self, activations, filter, img, learning_rate, opt_steps, verbose):\n",
    "        \n",
    "        size = img.shape[1]\n",
    "        img = torch.from_numpy(img.astype(np.float32).transpose(2,0,1))\n",
    "        \n",
    "        img = self.norm(img).double()\n",
    "        img_input = img.clone().detach().reshape(1, self.channels, size, size).to(self.device).requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([img_input], lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "        for n in range(opt_steps):\n",
    "            optimizer.zero_grad()\n",
    "            self.output = self.model(img_input)\n",
    "            # TODO: the idea is to find an input image that\n",
    "            #       'illuminate' ONLY ONE feature map (filter here)\n",
    "            # TODO: 1 test a loss function that punish current\n",
    "            #       activation filter with the rest of the\n",
    "            #       filters mean values in the layer\n",
    "            # TODO: 2 test a loss function that punish current activation\n",
    "            #       filter with all the rest of the filters mean value\n",
    "            #       of more layers (all?)\n",
    "            loss = -1 * activations.features[0, filter].mean()\n",
    "            loss.backward()\n",
    "            if verbose > 1:\n",
    "                print('.', end='')\n",
    "            #print(loss.clone().detach().cpu().item())\n",
    "            optimizer.step()\n",
    "        if verbose > 1:\n",
    "            print()\n",
    "        img = self.denorm(img_input.clone().detach()[0].type(torch.float32))\n",
    "        img = img.cpu().numpy().transpose(1,2,0)\n",
    "        return img\n",
    "        \n",
    "\n",
    "    def visualize(self, layer, filter, size=56, upscaling_steps=12, upscaling_factor=1.2, lr=0.1, opt_steps=20, blur=None, verbose=2):\n",
    "        training = self.model.training\n",
    "        self.model.eval()\n",
    "        self.model = self.model.double().to(self.device)\n",
    "        \n",
    "        # 1. generate random image\n",
    "        img = np.uint8(np.random.uniform(100, 160, (size, size, self.channels)))/255\n",
    "        \n",
    "        # 2. register hook\n",
    "        activations = SaveFeatures(self.layers[layer], self.device)\n",
    "        if verbose > 0:\n",
    "            print('Processing filter {}...'.format(filter))\n",
    "\n",
    "        for i in range(upscaling_steps): \n",
    "            if verbose > 1:\n",
    "                print('{:3d} x {:3d}'.format(size,size), end='')\n",
    "\n",
    "            img = self.optimize_img(activations, filter, img, learning_rate=lr, opt_steps=opt_steps, verbose=verbose)\n",
    "\n",
    "            if i < upscaling_steps-1:\n",
    "                size = int(size*upscaling_factor)\n",
    "                # scale the image up upscaling_steps times\n",
    "                img = cv2.resize(img, (size, size), interpolation = cv2.INTER_CUBIC)\n",
    "                # blur image to reduce high frequency patterns\n",
    "                if blur is not None: img = cv2.blur(img,(blur,blur))\n",
    "            img = np.clip(img, 0, 1)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print('preparing image...')\n",
    "        activations.close()\n",
    "        self.model.train(training)\n",
    "        if self.save != None:\n",
    "            self.save(\"layer_{:02d}_filter_{:03d}.jpg\".format(layer, filter), img)\n",
    "        return img\n",
    "    \n",
    "    # We return the mean of every activation value, but this could\n",
    "    # be other metric based on convolutional output values.\n",
    "    def get_activations(self, monitor, input, mean=True):\n",
    "\n",
    "        training = self.model.training\n",
    "        self.model.eval()\n",
    "        self.model = self.model.double().to(self.device)\n",
    "\n",
    "        activations = {}\n",
    "        mean_acts = {}\n",
    "\n",
    "        print('hooking layers {}'.format(monitor))\n",
    "        for layer in monitor:\n",
    "            activations[layer] = SaveFeatures(self.layers[layer], device=self.device)\n",
    "\n",
    "        self.output = self.model(input.to(self.device))\n",
    "\n",
    "        for layer in activations.keys():\n",
    "            filters = activations[layer].features.size()[1]\n",
    "            mean_acts[layer] = [activations[layer].features[0,i].mean().item() for i in range(filters)]\n",
    "\n",
    "        print('unhooking layers.')        \n",
    "        for layer in activations.keys():\n",
    "            activations[layer].close()\n",
    "            \n",
    "        self.model.train(training)\n",
    "        \n",
    "        if mean:\n",
    "            return mean_acts\n",
    "        \n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save all generated images in this directory\n",
    "images_dir = './images/'\n",
    "# create images directory\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save images \n",
    "def save(name, img):\n",
    "    global image_dir\n",
    "    plt.imsave(images_dir + name, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save in the variable 'monitor' every ReLU layers that appears\n",
    "# after every convolutional layer (they present non negative data)\n",
    "#monitor = [2,5,9,12,16,19,22,25,29,32,35,38,42,45,48,51]\n",
    "#monitor = [i for i, layer in enumerate(layers[0]) if isinstance(layer, torch.nn.Conv2d)]\n",
    "monitor = [i for i, layer in enumerate(layers[0]) if isinstance(layer, torch.nn.ReLU)]\n",
    "\n",
    "# define mean and std used for most famous images datasets\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
    "\n",
    "# define global transformations based on previous mean and std\n",
    "normalize = transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "denormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "\n",
    "# The input images will be prepared with this transformation\n",
    "# Minimum image size recommended for input is 224\n",
    "img2tensor = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), normalize])\n",
    "\n",
    "upscaling_steps=13\n",
    "opt_steps=20\n",
    "verbose=1\n",
    "TOP = 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing filter 91...\n",
      " 56 x  56....................\n",
      " 67 x  67....................\n",
      " 80 x  80....................\n",
      " 96 x  96....................\n",
      "115 x 115....................\n",
      "138 x 138....................\n",
      "165 x 165....................\n",
      "198 x 198....................\n",
      "237 x 237....................\n",
      "284 x 284....................\n",
      "340 x 340....................\n",
      "408 x 408....................\n",
      "489 x 489....................\n",
      "586 x 586....................\n",
      "703 x 703....................\n",
      "preparing image...\n"
     ]
    }
   ],
   "source": [
    "# Lets generate feature map visualization for filter 91 at layer 48\n",
    "# to discover the best (or a good one) 'texture image' that activate this filter\n",
    "fmv = FeatureMapVisualizer(model, device, layers_base=layers[0], save=None)\n",
    "\n",
    "steps = 15\n",
    "if str(device) == 'cpu': steps = 5\n",
    "    \n",
    "feature_map_48_91_img = fmv.visualize(\n",
    "                            layer=48, filter=91, size=56,\n",
    "                            upscaling_steps=steps, opt_steps=20, blur=3, verbose=2\n",
    "                        )\n",
    "plt.imsave(images_dir + 'input_for_L48F091.jpg', feature_map_48_91_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can now access the feature maps of layer `i`! The feature maps could i.e. have the shape `[1, 512, 7, 7]` where `1` is the batch dimension, `512` the number of filters/feature maps and `7` the height and width of the feature maps. **The goal is to maximize the average activation of a chosen feature map `j`.** \n",
    "\n",
    "We, therefore, define the following loss function: \n",
    "    `loss = -activations.features[0, j].mean()` and an optimizer \n",
    "    `optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)` that optimizes the pixel values. Optimizers by default minimize losses so instead of telling the optimizer to maximize the loss we simply multiply the mean activation by -1. Reset the gradients with `optimizer.zero_grad()`, calculate the gradients of the pixel values with `loss.backward()`, and change the pixel values with optimizer.step()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv2",
   "language": "python",
   "name": "ffcv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
