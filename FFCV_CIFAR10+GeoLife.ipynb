{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/sara.ebrahim-elkafrawy/.conda/envs/ffcv2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "cuda_ver = torch.version.cuda.replace(\".\", \"\")\n",
    "import time\n",
    "\n",
    "import composer\n",
    "from composer.models import ComposerResNetCIFAR\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(42) # For replicability\n",
    "\n",
    "\n",
    "from composer.datasets.ffcv_utils import ffcv_monkey_patches\n",
    "from composer.datasets.ffcv_utils import write_ffcv_dataset\n",
    "\n",
    "ffcv_monkey_patches()\n",
    "\n",
    "device = \"gpu\"\n",
    "batch_size = 32\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'113'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Loader\n",
    "Next, we instantiate our CIFAR10 dataset and dataloader. We'll use the Torchvision CIFAR10 and PyTorch dataloader for the sake of familiarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Normalization constants\n",
    "mean = (0.507, 0.487, 0.441)\n",
    "std = (0.267, 0.256, 0.276)\n",
    "\n",
    "batch_size = 1024\n",
    "num_workers = 2\n",
    "data_directory = \"/tmp\"\n",
    "\n",
    "cifar10_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(data_directory, train=True, download=True, transform=cifar10_transforms)\n",
    "test_dataset = datasets.CIFAR10(data_directory, train=False, download=True, transform=cifar10_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               num_workers=num_workers, \n",
    "                                               batch_size=batch_size,\n",
    "                                               pin_memory=True,\n",
    "                                               drop_last=True,\n",
    "                                               shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              num_workers=num_workers, \n",
    "                                              batch_size=batch_size,\n",
    "                                              pin_memory=True,\n",
    "                                              drop_last=False,\n",
    "                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Next, we create our model. We're using composer's built-in ResNet18. To use your own custom model, please see the [custom models tutorial](https://docs.mosaicml.com/en/stable/tutorials/adding_models_datasets.html#models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposerResNetCIFAR(model_name='resnet_20', num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and Scheduler\n",
    "The trainer will handle instantiating the optimizer, but first we need to create the optimizer and LR scheduler. We're using [MosaicML's SGD with decoupled weight decay](https://arxiv.org/abs/1711.05101):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(), # Model parameters to update\n",
    "    lr=0.05, # Peak learning rate\n",
    "    momentum=0.9,\n",
    "    weight_decay=2.0e-3 # If this looks large, it's because its not scaled by the LR as in non-decoupled weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the runtime short, we'll train our baseline model for five epochs. The first epoch will be linear warmup, followed by four epochs of constant LR. We achieve this by instantiating a `LinearWithWarmupScheduler` class. Feel free to increase the number of epochs in case you want to see the impact of running it for a longer duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = composer.optim.LinearWithWarmupScheduler(\n",
    "    t_warmup=\"1ep\", # Warm up over 1 epoch\n",
    "    alpha_i=1.0, # Flat LR schedule achieved by having alpha_i == alpha_f\n",
    "    alpha_f=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a baseline model\n",
    "And now we create our trainer: Note: We want to gpu as a device because FFCV works the best on GPU-capable machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 8.4.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n",
      "\n",
      "check self.logger\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [1]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [1]:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_epochs = \"5ep\" # Train for 5 epochs\n",
    "device = \"gpu\"\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=lr_scheduler,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train and measure the training time below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch     0 train 100%|█████████████████████████| 48/48 [00:13<00:00,  3.67ba/s, loss/train=1.5937]         \n",
      "\n",
      "Epoch     0 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     0 val    10%|██▌                      | 1/10 [00:00<00:02,  3.36ba/s]         \u001b[A\n",
      "Epoch     0 val    30%|███████▌                 | 3/10 [00:00<00:00,  8.03ba/s]         \u001b[A\n",
      "Epoch     0 val    50%|████████████▌            | 5/10 [00:00<00:00, 10.12ba/s]         \u001b[A\n",
      "Epoch     0 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 11.25ba/s]         \u001b[A\n",
      "Epoch     0 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 12.03ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 10/10 [00:00<00:00, 12.03ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 10/10 [00:00<00:00, 12.03ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 10/10 [00:00<00:00, 10.50ba/s, metrics/eval/Accuracy=0.3409]         \u001b[A\n",
      "Epoch     1 train 100%|█████████████████████████| 48/48 [00:04<00:00, 10.66ba/s, loss/train=1.1879]         \n",
      "\n",
      "Epoch     1 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     1 val    10%|██▌                      | 1/10 [00:00<00:02,  3.58ba/s]         \u001b[A\n",
      "Epoch     1 val    30%|███████▌                 | 3/10 [00:00<00:00,  7.77ba/s]         \u001b[A\n",
      "Epoch     1 val    50%|████████████▌            | 5/10 [00:00<00:00,  9.76ba/s]         \u001b[A\n",
      "Epoch     1 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 10.99ba/s]         \u001b[A\n",
      "Epoch     1 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 11.74ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.74ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.74ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 10/10 [00:00<00:00, 10.42ba/s, metrics/eval/Accuracy=0.2985]         \u001b[A\n",
      "Epoch     2 train 100%|█████████████████████████| 48/48 [00:04<00:00, 10.64ba/s, loss/train=0.9669]         \n",
      "\n",
      "Epoch     2 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     2 val    10%|██▌                      | 1/10 [00:00<00:02,  3.41ba/s]         \u001b[A\n",
      "Epoch     2 val    30%|███████▌                 | 3/10 [00:00<00:00,  7.70ba/s]         \u001b[A\n",
      "Epoch     2 val    50%|████████████▌            | 5/10 [00:00<00:00,  9.64ba/s]         \u001b[A\n",
      "Epoch     2 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 10.87ba/s]         \u001b[A\n",
      "Epoch     2 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 11.85ba/s]         \u001b[A\n",
      "Epoch     2 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.85ba/s]         \u001b[A\n",
      "Epoch     2 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.85ba/s]         \u001b[A\n",
      "Epoch     2 val   100%|█████████████████████████| 10/10 [00:00<00:00, 10.41ba/s, metrics/eval/Accuracy=0.5462]         \u001b[A\n",
      "Epoch     3 train 100%|█████████████████████████| 48/48 [00:04<00:00, 10.59ba/s, loss/train=0.8777]         \n",
      "\n",
      "Epoch     3 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     3 val    10%|██▌                      | 1/10 [00:00<00:02,  3.43ba/s]         \u001b[A\n",
      "Epoch     3 val    30%|███████▌                 | 3/10 [00:00<00:00,  7.54ba/s]         \u001b[A\n",
      "Epoch     3 val    50%|████████████▌            | 5/10 [00:00<00:00,  9.71ba/s]         \u001b[A\n",
      "Epoch     3 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 10.94ba/s]         \u001b[A\n",
      "Epoch     3 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 11.86ba/s]         \u001b[A\n",
      "Epoch     3 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.86ba/s]         \u001b[A\n",
      "Epoch     3 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.86ba/s]         \u001b[A\n",
      "Epoch     3 val   100%|█████████████████████████| 10/10 [00:00<00:00, 10.43ba/s, metrics/eval/Accuracy=0.5874]         \u001b[A\n",
      "Epoch     4 train 100%|█████████████████████████| 48/48 [00:04<00:00, 10.48ba/s, loss/train=0.7406]         \n",
      "\n",
      "Epoch     4 val     0%|                         | 0/10 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     4 val    10%|██▌                      | 1/10 [00:00<00:02,  3.45ba/s]         \u001b[A\n",
      "Epoch     4 val    30%|███████▌                 | 3/10 [00:00<00:00,  7.83ba/s]         \u001b[A\n",
      "Epoch     4 val    50%|████████████▌            | 5/10 [00:00<00:00,  9.92ba/s]         \u001b[A\n",
      "Epoch     4 val    70%|█████████████████▌       | 7/10 [00:00<00:00, 11.07ba/s]         \u001b[A\n",
      "Epoch     4 val    90%|██████████████████████▌  | 9/10 [00:00<00:00, 11.81ba/s]         \u001b[A\n",
      "Epoch     4 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.81ba/s]         \u001b[A\n",
      "Epoch     4 val   100%|█████████████████████████| 10/10 [00:00<00:00, 11.81ba/s]         \u001b[A\n",
      "Epoch     4 val   100%|█████████████████████████| 10/10 [00:00<00:00, 10.47ba/s, metrics/eval/Accuracy=0.6054]         \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 36.4631 seconds to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use FFCV dataloaders to Speed Up Training\n",
    "Next, we convert dataset to a format used by FFCV. FFCV uses it's own data format suitable for faster dataloading. Once this cell executes successfuly, you can find ```cifar_train.ffcv``` and ```cifar_val.ffcv``` in ```data_directory``` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.datasets.ffcv_utils import write_ffcv_dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "# Train dataset\n",
    "ds = CIFAR10(root=data_directory, train=True, download=True)\n",
    "write_ffcv_dataset(dataset=ds, write_path=data_directory + \"/cifar_train.ffcv\")\n",
    "\n",
    "# validation dataset\n",
    "ds = CIFAR10(root=data_directory, train=False, download=True)\n",
    "write_ffcv_dataset(dataset=ds, write_path=data_directory + \"/cifar_val.ffcv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current version of ffcv (0.0.3) has a bug where calling [len(dataloader) does shuffling](https://github.com/libffcv/ffcv/issues/163) of image indices to load, therefore, calls to len are expensive. Composer calls len(dataloader) function in training loop for every batch and, hence, this is a performance hit. We fix it by patching the len function using ffcv_monkey_patches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.datasets.ffcv_utils import ffcv_monkey_patches\n",
    "ffcv_monkey_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now let us construct FFCV train and test dataloaders. We use the similar transformations as used for TorchVision datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffcv\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "\n",
    "# Please note that this mean/std is different from the mean/std used for regular PyTorch dataloader as\n",
    "# ToTensor does the normalization for PyTorch dataloaders.\n",
    "cifar10_mean_ffcv = np.array([125.307, 122.961, 113.8575])\n",
    "cifar10_std_ffcv = np.array([51.5865, 50.847, 51.255])\n",
    "label_pipeline = [IntDecoder(), ffcv.transforms.ToTensor(), ffcv.transforms.Squeeze()]\n",
    "image_pipeline = [SimpleRGBImageDecoder(), ffcv.transforms.ToTensor(),\n",
    "                ffcv.transforms.ToTorchImage(channels_last=False, convert_back_int16=False),\n",
    "                ffcv.transforms.Convert(torch.float32),\n",
    "                transforms.Normalize(cifar10_mean_ffcv, cifar10_std_ffcv),\n",
    "            ]\n",
    "\n",
    "ffcv_train_dataloader = ffcv.Loader(\n",
    "                data_directory + \"/cifar_train.ffcv\",\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                order=ffcv.loader.OrderOption.RANDOM,\n",
    "                pipelines={\n",
    "                    'image': image_pipeline,\n",
    "                    'label': label_pipeline\n",
    "                },\n",
    "                drop_last=True,\n",
    "            )\n",
    "ffcv_test_dataloader = ffcv.Loader(\n",
    "                data_directory + \"/cifar_val.ffcv\",\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                order=ffcv.loader.OrderOption.RANDOM,\n",
    "                pipelines={\n",
    "                    'image': image_pipeline,\n",
    "                    'label': label_pipeline\n",
    "                },\n",
    "                drop_last=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now let's instantiate our model, optimizer, and trainer again but with FFCV dataloaders. No need to instantiate our scheduler again because it's stateless!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposerResNetCIFAR(model_name=\"resnet_20\", num_classes=10)\n",
    "\n",
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(),\n",
    "    lr=0.05,\n",
    "    momentum=0.9,\n",
    "    weight_decay=2.0e-3\n",
    ")\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=ffcv_train_dataloader,\n",
    "    eval_dataloader=ffcv_test_dataloader,\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=lr_scheduler,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "And let's get training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit()\n",
    "end_time = time.perf_counter()\n",
    "accelerated_time = end_time - start_time\n",
    "print(f\"It took {accelerated_time:0.4f} seconds to train with FFCV dataloaders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoLife dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "\n",
    "CURR_DIR = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "PARENT_DIR = os.path.dirname(CURR_DIR)\n",
    "sys.path.insert(0, CURR_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.ffcv_loader.dataset_ffcv import GeoLifeCLEF2022DatasetFFCV\n",
    "from dataset.pytorch_dataset import GeoLifeCLEF2022Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/mila/s/sara.ebrahim-elkafrawy/scratch/ecosystem_project/tmp_geo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoLife + Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GeoLifeCLEF2022Dataset(\n",
    "    \"/network/scratch/s/sara.ebrahim-elkafrawy\",\n",
    "    \"train\",\n",
    "    region=\"both\",\n",
    "    patch_data=\"all\", # self.opts.data.bands,\n",
    "    use_rasters=False,\n",
    "    patch_extractor=None,\n",
    "    transform=None,\n",
    "    target_transform=None,\n",
    "    )\n",
    "\n",
    "val_dataset = GeoLifeCLEF2022Dataset(\n",
    "        \"/network/scratch/s/sara.ebrahim-elkafrawy\",\n",
    "        \"val\",\n",
    "        region=\"both\",\n",
    "        patch_data=\"all\", #self.opts.data.bands,\n",
    "        use_rasters=False,\n",
    "        patch_extractor=None,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               num_workers=num_workers, \n",
    "                                               batch_size=batch_size,\n",
    "                                               pin_memory=True,\n",
    "                                               drop_last=True,\n",
    "                                               shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                              num_workers=num_workers, \n",
    "                                              batch_size=batch_size,\n",
    "                                              pin_memory=True,\n",
    "                                              drop_last=False,\n",
    "                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposerResNetCIFAR(model_name='resnet_20', num_classes=17037)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(), # Model parameters to update\n",
    "    lr=0.05, # Peak learning rate\n",
    "    momentum=0.9,\n",
    "    weight_decay=2.0e-3 # If this looks large, it's because its not scaled by the LR as in non-decoupled weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.utils.reproducibility:Setting seed to 2359799908\n",
      "INFO:composer.trainer.trainer:Run name: 1658191072-fair-oarfish\n",
      "INFO:composer.trainer.trainer:Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.\n",
      "INFO:composer.trainer.trainer:Setting seed to 2359799908\n",
      "INFO:composer.utils.reproducibility:Setting seed to 2359799908\n"
     ]
    }
   ],
   "source": [
    "train_epochs = \"2ep\"\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=val_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "#     schedulers=lr_scheduler,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoLife + FFCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffcv\n",
    "import torch\n",
    "from ffcv.fields.decoders import (\n",
    "    IntDecoder,\n",
    "    NDArrayDecoder,\n",
    "    SimpleRGBImageDecoder,\n",
    "    CenterCropRGBImageDecoder,\n",
    ")\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import (\n",
    "    RandomHorizontalFlip,\n",
    "    Cutout,\n",
    "    NormalizeImage,\n",
    "    RandomTranslate,\n",
    "    Convert,\n",
    "    ToDevice,\n",
    "    ToTensor,\n",
    "    ToTorchImage,\n",
    "    ImageMixup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffcv_train_dataset = GeoLifeCLEF2022DatasetFFCV(\n",
    "    \"/network/scratch/s/sara.ebrahim-elkafrawy\",\n",
    "    \"train\",\n",
    "    region=\"both\",\n",
    "    patch_data=\"all\", # self.opts.data.bands,\n",
    "    use_rasters=False,\n",
    "    patch_extractor=None,\n",
    "    transform=None,\n",
    "    target_transform=None,\n",
    "    )\n",
    "\n",
    "train_write_path = os.path.join(\n",
    "       save_dir , \"geolife_train_data.ffcv\"\n",
    "    )\n",
    "\n",
    "ffcv_val_dataset = GeoLifeCLEF2022DatasetFFCV(\n",
    "        \"/network/scratch/s/sara.ebrahim-elkafrawy\",\n",
    "        \"val\",\n",
    "        region=\"both\",\n",
    "        patch_data=\"all\", #self.opts.data.bands,\n",
    "        use_rasters=False,\n",
    "        patch_extractor=None,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    )\n",
    "\n",
    "val_write_path = os.path.join(\n",
    "        save_dir, \"geolife_val_data.ffcv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.datasets.ffcv_utils:Writing dataset in FFCV <file>.ffcv format to /home/mila/s/sara.ebrahim-elkafrawy/scratch/ecosystem_project/tmp_geo/geo_val.ffcv.\n",
      "100%|██████████| 256/256 [00:01<00:00, 212.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# write_ffcv_dataset(dataset=ffcv_train_dataset, write_path=save_dir + \"/geo_train.ffcv\")\n",
    "write_ffcv_dataset(dataset=ffcv_val_dataset, write_path=save_dir + \"/geo_val.ffcv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pipeline = [IntDecoder(), ffcv.transforms.ToTensor(), ffcv.transforms.Squeeze()]\n",
    "image_pipeline = [SimpleRGBImageDecoder(), ffcv.transforms.ToTensor(),\n",
    "                ffcv.transforms.ToTorchImage(channels_last=False, convert_back_int16=False),\n",
    "                ffcv.transforms.Convert(torch.float32),\n",
    "                transforms.Normalize(\n",
    "                    np.array([106.9413, 114.8729, 104.5280]),\n",
    "                    np.array([51.0005, 44.8594, 43.2014]),),\n",
    "            ]\n",
    "\n",
    "ffcv_train_dataloader = ffcv.Loader(\n",
    "                save_dir + \"/geo_train.ffcv\",\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                order=ffcv.loader.OrderOption.RANDOM,\n",
    "                pipelines={\n",
    "                    'image': image_pipeline,\n",
    "                    'label': label_pipeline\n",
    "                },\n",
    "                drop_last=True,\n",
    "            )\n",
    "ffcv_val_dataloader = ffcv.Loader(\n",
    "                save_dir + \"/geo_val.ffcv\",\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                order=ffcv.loader.OrderOption.RANDOM,\n",
    "                pipelines={\n",
    "                    'image': image_pipeline,\n",
    "                    'label': label_pipeline\n",
    "                },\n",
    "                drop_last=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(val_dataloader), type(ffcv_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2 = next(iter(ffcv_val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposerResNetCIFAR(model_name='resnet_20', num_classes=17037)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = composer.optim.DecoupledSGDW(\n",
    "    model.parameters(), # Model parameters to update\n",
    "    lr=0.05, # Peak learning rate\n",
    "    momentum=0.9,\n",
    "    weight_decay=2.0e-3 # If this looks large, it's because its not scaled by the LR as in non-decoupled weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.utils.reproducibility:Setting seed to 1769560530\n",
      "INFO:composer.trainer.trainer:Run name: 1658191086-famous-kagu\n",
      "INFO:composer.trainer.trainer:Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.\n",
      "INFO:composer.trainer.trainer:Setting seed to 1769560530\n",
      "INFO:composer.utils.reproducibility:Setting seed to 1769560530\n"
     ]
    }
   ],
   "source": [
    "train_epochs = \"2ep\"\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=ffcv_val_dataloader,\n",
    "    eval_dataloader=ffcv_val_dataloader,\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "#     schedulers=lr_scheduler,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:composer.trainer.trainer:Using precision Precision.FP32\n",
      "Epoch     0 train 100%|█████████████████████████| 8/8 [00:09<00:00,  1.14s/ba, loss/train=9.8233]         \n",
      "\n",
      "Epoch     0 val     0%|                         | 0/8 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     0 val    25%|██████▎                  | 2/8 [00:00<00:00, 15.83ba/s]         \u001b[A\n",
      "Epoch     0 val    62%|███████████████▋         | 5/8 [00:00<00:00, 20.93ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 8/8 [00:00<00:00, 22.69ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 8/8 [00:00<00:00, 22.69ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 8/8 [00:00<00:00, 22.69ba/s]         \u001b[A\n",
      "Epoch     0 val   100%|█████████████████████████| 8/8 [00:00<00:00, 20.56ba/s, metrics/eval/Accuracy=0.0000]         \u001b[A\n",
      "Epoch     1 train 100%|█████████████████████████| 8/8 [00:01<00:00,  5.85ba/s, loss/train=8.8551]         \n",
      "\n",
      "Epoch     1 val     0%|                         | 0/8 [00:00<?, ?ba/s]         \u001b[A\n",
      "Epoch     1 val    25%|██████▎                  | 2/8 [00:00<00:00, 18.44ba/s]         \u001b[A\n",
      "Epoch     1 val    62%|███████████████▋         | 5/8 [00:00<00:00, 22.26ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 8/8 [00:00<00:00, 23.56ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 8/8 [00:00<00:00, 23.56ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 8/8 [00:00<00:00, 23.56ba/s]         \u001b[A\n",
      "Epoch     1 val   100%|█████████████████████████| 8/8 [00:00<00:00, 22.26ba/s, metrics/eval/Accuracy=0.0078]         \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 12.0103 seconds to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.fit()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"It took {end_time - start_time:0.4f} seconds to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "245.5064 +  269.8520 / 2 seconds\n",
    "886.5012 It took 894.2513 seconds to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PL model:\n",
    "ffcv ->    10 epochs 58.09489850606769\n",
    "no   ->    10 epochs 35.13264705892652    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv2",
   "language": "python",
   "name": "ffcv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
