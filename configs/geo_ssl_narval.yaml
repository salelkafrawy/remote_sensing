log_wandb: true
wandb:
  project_name: "geo-ssl"
  mode: "offline"
  tags: []
    
max_epochs: 200 # null
gpus: 4
use_ffcv_loader: False   #you have to use ["rgb", "near_ir"] in the data.bands !!

task: "ssl"  # options: [base, multitask, seco, feats, ssl]
freeze: False # in the case of using the whole pre-trained network as feature extractor

num_species: 17037

loss: "CrossEntropy" # options: [CrossEntropy, PolyLoss]


module:
  model: "seco_resnet50_1m" #options: [vgg16, resnet18, resnet50, seco_resnet18_1m, seco_resnet50_1m, seco_resnet18_100k, seco_resnet50_100k]
  #fc: "linear"
  pretrained: true
  lr: 0.01 # best lr = 0.3311311214825908
  
  
ssl:
  learning_rate: 0.2
  ssl_pretrained: False
  num_keys: 1
  schedule: [120, 160] #  During training, the learning rate is divided by 10 at 60% and 80% of the epochs
  base_encoder: 'resnet50'
  emb_dim: 128
  num_negatives: 16384
  encoder_momentum: 0.999
  softmax_temperature: 0.07
  momentum: 0.9
  weight_decay: 1e-4
  use_ddp: True
  use_ddp2: False
  accelerator: gpu
  strategy: ddp # options [ddp, horovod]
  devices: 1
  num_nodes: 1
  online_max_epochs: 25
  online_val_every_n_epoch: 25
  
  
data:
  loaders:
    num_workers: 8   # 32
    batch_size: 64 # 256
  datatype: "img"   #refl or img 
  bands:  ["rgb"]  # available options: ["rgb", "near_ir", "landcover", "altitude", "all"]
  splits:  # available options: [train, train+val, val, test]
    train: train  #  `val` for debugging purposes . should be train+val
    val: val
    test: test  
  transforms:
    - name: crop
      ignore: True
      p: 0.5
      center: true # disable randomness, crop around the image's center
      size: [256, 256]
    - name: resize
      ignore: False
      size: [224,224]
    - name: hflip
      ignore: "val"
      p: 0.5
    - name: vflip
      ignore: "val"
      p: 0.5    
    - name: normalize
      ignore: False
      means: [106.94150444, 114.87315837, 104.52826283]   # [0.4194, 0.4505, 0.4099], 
      std: [50.59516823, 44.13010964, 41.84300729]        # [0.20, 0.1759, 0.1694]
      band: "rgb"
    - name: normalize
      ignore: True
      means: [131.0458]     # 0.5139
      std: [53.0884]        # 0.2082
      band: "near_ir"
    - name: normalize
      ignore: True
      means: [298.1693]
      std: [459.3285]
      band: "altitude"
    - name: normalize
      ignore: True
      means: [17.4200]
      std: [9.5173]
      band: "landcover"
      
  
  
scheduler:   # default interval in lighting is "epoch"
  name: "CosineRestarts" #"ReduceLROnPlateau" "StepLR" "CosineRestarts" "OneCycleLR" "CosineResDecay"
  reduce_lr_plateau: 
    factor: 0.5
    lr_schedule_patience: 5
  step_lr:
    step_size: 100
    gamma: 0.5
  warmup:
    warmup_epochs: 10
  cosine:
    epochs: 10          # or t_0: How many epochs/steps should pass between each restart (in 10 epochs/steps it will restart)   Note: the choice from epoch and step is based on how the lightning scheduler's "interval" is set
    t_mult: 1        #  A factor increases T_i (T_0 at the beginning) after a restart
    eta_min: 0.001
    last_epoch: -1   # T_curr: how many epochs have been performed since the last restart
  cosine_decay:
    epochs: 10
    t_mult: 1
    eta_min: 0.001
    last_epoch: -1
    decay: 0.9
  one_cycle:  #  anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. initial_lr = max_lr/div_factor (default is 25)
    max_lr: 0.7


      
optimizer: "SGD" #"Adam"

nesterov: True
momentum: 0.9
dampening: 0.15


#auto lr will only work if there is only one optimizer
auto_lr_find: True

losses:
#scale attribute is just for plotting if the values are very small 

#   criterion: "CrossEntropy" #or MAE or MSE  (loss to choosefor optim )
#   ce:
#     ignore: False
#       #weights on the cross entropy
#     lambd_pres: 10
#     lambd_abs: 1
  metrics:
    - name: topk-error
      ignore: False
      k: 30
      scale: 1
#     - name: topk-accuracy
#       ignore: False
#       k: 30
#       scale: 1
