log_wandb: true
wandb:
  project_name: "rs-downstream"
  mode: "online"
  tags: []

max_epochs: 300 # null
gpus: 1
use_ffcv_loader: False   #you have to use ["rgb", "near_ir"] in the data.bands !!

task: "mosaiks"  # options: [base, multitask, seco, feats, ssl, mosaiks]

num_species: 17037

loss: "CrossEntropy" # options: [CrossEntropy, PolyLoss]

mosaiks:
  model_name: "one_layer" # options["one_layer", "two_layers"]
  finetune: False         # options [True, False (linear-probe)]
  conv1_num_filters: 100 #100  512
  conv2_num_filters: 64  #64   128
  num_final_feats: 5184  # for the first trial: 5184 . second 10368
  pool_size: 2
  pool_stride: 2
  adaptive_pool_sz: 5
  patch_size: 7
  in_channels: 3
  conv_bias: True
  bias: 0.0
  
  
module:
  model: "resnet50"  #options: [vgg16, resnet18, resnet50, seco_resnet18_1m, seco_resnet50_1m, seco_resnet18_100k, seco_resnet50_100k] 
  #fc: "linear"
  multimodal: False
  pretrained: true
  lr: 0.1 # best lr = 0.3311311214825908
  
        
scheduler:   # default interval in lighting is "epoch"
  name: "CosineRestarts" #"ReduceLROnPlateau" "StepLR" "CosineRestarts" "OneCycleLR" "CosineResDecay"
  reduce_lr_plateau: 
    factor: 0.5
    lr_schedule_patience: 5
  step_lr:
    step_size: 100
    gamma: 0.5
  warmup:
    warmup_epochs: 10
  cosine:
    epochs: 10          # or t_0: How many epochs/steps should pass between each restart (in 10 epochs/steps it will restart)   Note: the choice from epoch and step is based on how the lightning scheduler's "interval" is set
    t_mult: 1        #  A factor increases T_i (T_0 at the beginning) after a restart
    eta_min: 0.001
    last_epoch: -1   # T_curr: how many epochs have been performed since the last restart
  cosine_decay:
    epochs: 10
    t_mult: 1
    eta_min: 0.001
    last_epoch: -1
    decay: 0.9
  one_cycle:  #  anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. initial_lr = max_lr/div_factor (default is 25)
    max_lr: 0.7


data:
  loaders:
    num_workers: 4
    batch_size: 32
  datatype: "img"   #refl or img 
  bands:  ["rgb"]   # available options: ["rgb", "near_ir", "landcover", "altitude", "all"]
  splits:  # available options: [train, train+val, val, test]
    train: train  #  `val` for debugging purposes . should be train+val
    val: val
    test: test  
  transforms:
    - name: whiten
      ignore: True
      num_feats: 3
      num_groups: 3
      momentum: 1.
    - name: crop
      ignore: True
      p: 0.5
      center: true # disable randomness, crop around the image's center
      size: [256, 256]
    - name: resize
      ignore: False
      size: [224,224]
    - name: hflip
      ignore: "val"
      p: 0.5
    - name: vflip
      ignore: "val"
      p: 0.5    
    - name: normalize
      ignore: False
      means: [106.9413, 114.8733, 104.5285] 
      std: [51.0005, 44.8595, 43.2014] 
      band: "rgb"
    - name: normalize
      ignore: True
      means: [131.0458]
      std: [53.0884]
      band: "near_ir"
    - name: normalize
      ignore: True
      means: [298.1693]
      std: [459.3285]
      band: "altitude"
    - name: normalize
      ignore: True
      means: [17.4200]
      std: [9.5173]
      band: "landcover"
      
optimizer: "SGD" #  options: ["AdamW", "SGD"]

nesterov: True
momentum: 0.9
dampening: 0.15


#auto lr will only work if there is only one optimizer
auto_lr_find: True

losses:
#scale attribute is just for plotting if the values are very small 

#   criterion: "CrossEntropy" #or MAE or MSE  (loss to choosefor optim )
#   ce:
#     ignore: False
#       #weights on the cross entropy
#     lambd_pres: 10
#     lambd_abs: 1
  metrics:
    - name: topk-error
      ignore: False
      k: 30
      scale: 1
#     - name: topk-accuracy
#       ignore: False
#       k: 30
#       scale: 1
